{"cells":[{"cell_type":"code","execution_count":270,"metadata":{"id":"3DctCqcPZpwa","executionInfo":{"status":"ok","timestamp":1726847506516,"user_tz":-360,"elapsed":465,"user":{"displayName":"Nafiul Islam","userId":"03319937839838894668"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from collections import Counter  # Make sure this import is present\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from collections import Counter\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, average_precision_score"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fOwEoSmC7AJb","executionInfo":{"status":"ok","timestamp":1726847511288,"user_tz":-360,"elapsed":4284,"user":{"displayName":"Nafiul Islam","userId":"03319937839838894668"}},"outputId":"ab108f6e-1936-4d3c-d8fe-5dad1e52464f"},"execution_count":271,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","metadata":{"id":"s1Ae2MMSZpwb"},"source":["# Telco Preprocess\n"]},{"cell_type":"markdown","metadata":{"id":"vgJIzveXZpwc"},"source":[]},{"cell_type":"code","execution_count":272,"metadata":{"id":"8PNS1ZvZZpwc","executionInfo":{"status":"ok","timestamp":1726847511288,"user_tz":-360,"elapsed":17,"user":{"displayName":"Nafiul Islam","userId":"03319937839838894668"}}},"outputs":[],"source":["def preprocess_dataset_telco(filepath, target_col):\n","    \"\"\"\n","    Generic preprocessing function for a dataset.\n","    - Handles missing values, duplicates, encoding, and scaling.\n","\n","    Parameters:\n","    - filepath: Path to the CSV file.\n","    - target_col: Name of the target column.\n","\n","    Returns:\n","    - X_train, X_test, y_train, y_test: Processed dataset split into training and testing sets.\n","    \"\"\"\n","    # Load the dataset\n","    df = pd.read_csv(filepath)\n","\n","    print(f\"Initial shape: {df.shape}\")\n","\n","    # Convert 'TotalCharges' (or any other numerical column with spaces) to NaN, then to numeric\n","    df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n","\n","    # Report missing values and duplicates\n","    missing_count = df.isna().sum().sum()\n","    duplicate_count = df.duplicated().sum()\n","    print(f\"Missing values: {missing_count}\")\n","    print(f\"Duplicated values: {duplicate_count}\")\n","\n","    # Drop rows with missing target values (Churn)\n","    df = df.dropna(subset=[target_col])\n","\n","    # Drop duplicated rows\n","    df = df.drop_duplicates()\n","\n","    # Fill numeric missing values with the mean of respective columns\n","    num_cols = df.select_dtypes(include=['float64', 'int64']).columns\n","    df[num_cols] = df[num_cols].fillna(df[num_cols].mean())\n","\n","    # Fill categorical missing values with the mode of respective columns\n","    cat_cols = df.select_dtypes(include=['object']).columns\n","    df[cat_cols] = df[cat_cols].fillna(df[cat_cols].mode().iloc[0])\n","\n","    # Drop irrelevant columns (like 'customerID')\n","    if 'customerID' in df.columns:\n","        df = df.drop(columns=['customerID'])\n","\n","    # Identify binary and nominal columns\n","    binary_cols = df.columns[df.nunique() == 2].tolist()\n","    nominal_cols = df.select_dtypes(include=['object']).columns.difference(binary_cols).tolist()\n","\n","    print(f\"Binary columns: {binary_cols}\")\n","    print(f\"Nominal columns: {nominal_cols}\")\n","\n","    # Encode binary columns using LabelEncoder\n","    for col in binary_cols:\n","        df[col] = LabelEncoder().fit_transform(df[col])\n","\n","    # One-hot encode nominal columns\n","    df = pd.get_dummies(df, columns=nominal_cols)\n","\n","    print(f\"Shape after encoding: {df.shape}\")\n","\n","    # Scale the numeric columns except for binary columns\n","    scale_cols = [col for col in num_cols if col not in binary_cols]\n","    print(f\"Columns to scale: {scale_cols}\")\n","\n","    df[scale_cols] = StandardScaler().fit_transform(df[scale_cols])\n","\n","    # Separate features and target\n","    X = df.drop(columns=[target_col])\n","    y = df[target_col]\n","    X = np.array(X)\n","    y = np.array(y)\n","    # Split the dataset into training and testing sets\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","    return X_train, X_test, y_train, y_test"]},{"cell_type":"markdown","source":["# Adult Preprocess"],"metadata":{"id":"AhiItg910FqL"}},{"cell_type":"code","execution_count":273,"metadata":{"id":"Y50jEApzZpwc","executionInfo":{"status":"ok","timestamp":1726847511288,"user_tz":-360,"elapsed":16,"user":{"displayName":"Nafiul Islam","userId":"03319937839838894668"}}},"outputs":[],"source":["def prepare_adult_data(file_path):\n","    \"\"\"\n","    Function to preprocess the Adult dataset for classification.\n","\n","    Parameters:\n","    - file_path: Path to the dataset file.\n","\n","    Returns:\n","    - X_train, X_test, y_train, y_test: Preprocessed data split into training and testing sets.\n","    \"\"\"\n","    # Define the column headers based on the dataset description\n","    columns = [\n","        'age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status',\n","        'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss',\n","        'hours-per-week', 'native-country', 'income'\n","    ]\n","\n","    # Load the dataset, handle missing values and whitespace issues\n","    adult_data = pd.read_csv(file_path, header=None, sep=',\\s', na_values='?', engine='python', names=columns)\n","\n","    # Display initial shape of the data\n","    print(f\"Original data dimensions: {adult_data.shape}\")\n","\n","    # Count missing and duplicated values\n","    total_missing = adult_data.isnull().sum().sum()\n","    total_duplicates = adult_data.duplicated().sum()\n","\n","    print(f\"Total missing entries: {total_missing}\")\n","    print(f\"Total duplicate entries: {total_duplicates}\")\n","\n","    # Remove rows with missing target values and drop duplicates\n","    adult_data = adult_data.dropna(subset=['income']).drop_duplicates()\n","\n","    # Clean the target column ('income') and map labels\n","    adult_data['income'] = adult_data['income'].str.strip().map({'>50K': 0, '<=50K': 1})\n","\n","    # Fill missing numeric values with the column mean\n","    numeric_cols = adult_data.select_dtypes(include=['number']).columns\n","    adult_data[numeric_cols] = adult_data[numeric_cols].fillna(adult_data[numeric_cols].mean())\n","\n","    # Fill missing categorical values with the mode\n","    categorical_cols = adult_data.select_dtypes(exclude=['number']).columns\n","    adult_data[categorical_cols] = adult_data[categorical_cols].fillna(adult_data[categorical_cols].mode().iloc[0])\n","\n","    # Identify binary columns and nominal columns\n","    binary_cols = [col for col in adult_data.columns if adult_data[col].nunique() == 2]\n","    categorical_cols = adult_data.select_dtypes(include=['object']).columns.difference(binary_cols)\n","\n","    print(f\"Binary columns: {binary_cols}\")\n","    print(f\"Nominal columns: {list(categorical_cols)}\")\n","\n","    # Apply label encoding to binary columns\n","    le = LabelEncoder()\n","    for col in binary_cols:\n","        adult_data[col] = le.fit_transform(adult_data[col])\n","\n","    # One-hot encoding for nominal categorical columns\n","    adult_data = pd.get_dummies(adult_data, columns=categorical_cols)\n","\n","    # Scaling numeric columns except binary columns\n","    scale_columns = [col for col in numeric_cols if col not in binary_cols]\n","    print(f\"Columns to be scaled: {scale_columns}\")\n","\n","    scaler = StandardScaler()\n","    adult_data[scale_columns] = scaler.fit_transform(adult_data[scale_columns])\n","\n","    # Split dataset into features and target\n","    features = adult_data.drop(columns=['income'])\n","    target = adult_data['income']\n","\n","    # Split the dataset into training and testing sets\n","    features = np.array(features)\n","    target = np.array(target)\n","    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n","\n","    return X_train, X_test, y_train, y_test"]},{"cell_type":"markdown","source":["# Credit Preprocess"],"metadata":{"id":"oTnmZWSi0K4J"}},{"cell_type":"markdown","metadata":{"id":"4qa-UY7sZpwd"},"source":[]},{"cell_type":"code","source":["def preprocess_credit_data():\n","    # Load dataset\n","    credit_data = pd.read_csv(\"creditcard.csv\")\n","\n","    # Display dataset shape\n","    print(f\"Dataset dimensions: {credit_data.shape}\")\n","\n","    # Missing values check\n","    missing_values_count = credit_data.isnull().sum().sum()\n","    print(f\"Total missing values: {missing_values_count}\")\n","\n","    # Duplicated values check\n","    duplicated_count = credit_data.duplicated().sum()\n","    print(f\"Duplicated rows: {duplicated_count}\")\n","\n","    # Remove rows where 'Class' is missing\n","    credit_data.dropna(subset=['Class'], inplace=True)\n","\n","    # Remove duplicate entries\n","    credit_data.drop_duplicates(inplace=True)\n","\n","    # Fill missing values with column mean (only for numeric columns)\n","    num_columns = credit_data.select_dtypes(include='number').columns\n","    credit_data[num_columns] = credit_data[num_columns].fillna(credit_data[num_columns].mean())\n","    binary_cols = [column for column in credit_data.columns if credit_data[column].nunique() <= 2]\n","    nominal_cols = [column for column in credit_data.columns if credit_data[column].dtype == 'object' and column not in binary_cols]\n","\n","    print(f\"Binary columns: {binary_cols}\")\n","    print(f\"Nominal columns: {nominal_cols}\")\n","    numeric_cols = credit_data.select_dtypes(include=['float64', 'int64']).columns\n","    scale_cols = [col for col in numeric_cols if col not in binary_cols]\n","\n","    print(f\"Columns to scale: {scale_cols}\")\n","    scaler = StandardScaler()\n","    credit_data[scale_cols] = scaler.fit_transform(credit_data[scale_cols])\n","\n","    # Convert boolean columns to integers\n","    bool_cols = credit_data.select_dtypes(include=['bool']).columns\n","    credit_data[bool_cols] = credit_data[bool_cols].astype(int)\n","\n","    # Separate positive and negative 'Class' samples\n","    positive_samples = credit_data[credit_data['Class'] == 1]\n","    negative_samples = credit_data[credit_data['Class'] == 0].sample(n=20000, random_state=42)\n","\n","    # Combine positive and sampled negative samples\n","    dataset_subset = pd.concat([positive_samples, negative_samples])\n","\n","    print(f\"Subset dimensions: {dataset_subset.shape}\")\n","    X = dataset_subset.drop(columns=['Class'])\n","    y = dataset_subset['Class']\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","    # Further split training set into training and validation sets\n","    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n","\n","    return X_train, X_test, y_train, y_test\n","\n","# Execute the preprocessing function\n"],"metadata":{"id":"1EuIaDnyNo_M","executionInfo":{"status":"ok","timestamp":1726847511289,"user_tz":-360,"elapsed":16,"user":{"displayName":"Nafiul Islam","userId":"03319937839838894668"}}},"execution_count":274,"outputs":[]},{"cell_type":"code","execution_count":275,"metadata":{"id":"4UivZxFdZpwe","executionInfo":{"status":"ok","timestamp":1726847511289,"user_tz":-360,"elapsed":15,"user":{"displayName":"Nafiul Islam","userId":"03319937839838894668"}}},"outputs":[],"source":["# from sklearn.preprocessing import LabelEncoder\n","# encoder = LabelEncoder()\n","# target = encoder.fit_transform(target)\n","# for i in features:\n","#     if features[i].dtypes == 'object':\n","#         features[i] = features[i].astype('category')\n","# features = pd.get_dummies(features)\n","# features\n"]},{"cell_type":"markdown","metadata":{"id":"Dbc5S_l0Zpwe"},"source":["# Scaling"]},{"cell_type":"code","execution_count":276,"metadata":{"id":"04-_pXVWZpwe","executionInfo":{"status":"ok","timestamp":1726847511289,"user_tz":-360,"elapsed":14,"user":{"displayName":"Nafiul Islam","userId":"03319937839838894668"}}},"outputs":[],"source":["# from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","# def scaller(type):\n","#     lst = []\n","#     for i in features:\n","#         if features[i].dtypes != 'bool':\n","#             lst.append(i)\n","#     sc = StandardScaler()\n","#     if type == 'std':\n","#         sc = StandardScaler()\n","#     else:\n","#        sc = MinMaxScaler()\n","#     features[lst] = sc.fit_transform(features[lst])\n","\n","# scaller('minmax')\n","# features"]},{"cell_type":"markdown","metadata":{"id":"Xf-vYs3jZpwf"},"source":["# Correlation Analysis"]},{"cell_type":"code","execution_count":277,"metadata":{"id":"RA-QfWRFZpwf","executionInfo":{"status":"ok","timestamp":1726847511289,"user_tz":-360,"elapsed":14,"user":{"displayName":"Nafiul Islam","userId":"03319937839838894668"}}},"outputs":[],"source":["# features_df = pd.DataFrame(features, columns=features.columns)\n","# target_df = pd.DataFrame(target, columns=['Attrition'])"]},{"cell_type":"code","execution_count":278,"metadata":{"id":"lxssCBuTZpwf","executionInfo":{"status":"ok","timestamp":1726847511289,"user_tz":-360,"elapsed":13,"user":{"displayName":"Nafiul Islam","userId":"03319937839838894668"}}},"outputs":[],"source":["# # Correlation analysis of features with target\n","# target_series = target_df['Attrition']\n","# correlations = features.corrwith(target_series)\n","# correlations"]},{"cell_type":"code","execution_count":279,"metadata":{"id":"oelgWMv4Zpwf","executionInfo":{"status":"ok","timestamp":1726847511289,"user_tz":-360,"elapsed":13,"user":{"displayName":"Nafiul Islam","userId":"03319937839838894668"}}},"outputs":[],"source":["# correlation_matrix = features_df.corr()\n","# correlation_matrix"]},{"cell_type":"code","execution_count":280,"metadata":{"id":"sxO8GQ49Zpwf","executionInfo":{"status":"ok","timestamp":1726847511290,"user_tz":-360,"elapsed":13,"user":{"displayName":"Nafiul Islam","userId":"03319937839838894668"}}},"outputs":[],"source":["# my_20_features = features[abs(correlations).sort_values(ascending=False).head(20).index]\n","# my_20_features"]},{"cell_type":"markdown","metadata":{"id":"Nz_7faraZpwf"},"source":["# 1D scatter plot"]},{"cell_type":"code","execution_count":281,"metadata":{"id":"ZHT3f8u-Zpwf","executionInfo":{"status":"ok","timestamp":1726847511290,"user_tz":-360,"elapsed":12,"user":{"displayName":"Nafiul Islam","userId":"03319937839838894668"}}},"outputs":[],"source":["\n","# import matplotlib.pyplot as plt\n","# import numpy as np\n","# class_0 = my_20_features.loc[target_df[\"Attrition\"] == 0]\n","# class_1 = my_20_features.loc[target_df[\"Attrition\"] == 1]\n","\n","# # for column in my_20_features:\n","# #     plt.plot(class_0[column], np.zeros_like(class_0[column]), 'o', label='Class 0')\n","# #     plt.plot(class_1[column], np.zeros_like(class_1[column]), 'o', label='Class 1')\n","# #     plt.legend()\n","# #     plt.xlabel(column)\n","# #     plt.title(\"1D Scatter plot of \" + column + \" by Numeric classes\")\n","# #     plt.show()\n","\n"]},{"cell_type":"markdown","source":["# Bagging"],"metadata":{"id":"hfoGqYoXIlHd"}},{"cell_type":"code","source":["\n","\n","class MyBaggingClassifier:\n","    def __init__(self, base_estimator, n_estimators=9):\n","        self.base_estimator = base_estimator\n","        self.n_estimators = n_estimators\n","        self.models = []\n","\n","    def _bootstrap_sample(self, X, y):\n","        \"\"\"Create a bootstrap sample of the dataset.\"\"\"\n","        n_samples = X.shape[0]\n","        indices = np.random.choice(n_samples, size=n_samples, replace=True)\n","        return X[indices], y[indices]\n","\n","    def fit(self, X, y):\n","        \"\"\"Train the bagging ensemble of models.\"\"\"\n","        self.models = []\n","        for _ in range(self.n_estimators):\n","            # Generate bootstrap sample\n","            X_sample, y_sample = self._bootstrap_sample(X, y)\n","\n","            # Train a new model on the bootstrap sample\n","            model = self.base_estimator()\n","            model.fit(X_sample, y_sample)\n","\n","            # Store the trained model\n","            self.models.append(model)\n","\n","    def predict(self, X):\n","        \"\"\"Make predictions using the ensemble of models.\"\"\"\n","        # Collect predictions from each model\n","        predictions = np.array([model.predict(X) for model in self.models])\n","\n","        # Majority voting: the most common prediction at each instance\n","        final_predictions = [Counter(predictions[:, i]).most_common(1)[0][0] for i in range(X.shape[0])]\n","        return np.array(final_predictions)\n","\n","    def predict_proba(self, X):\n","        X = np.asarray(X, dtype=np.float64)\n","        \"\"\"Get probability estimates from each model.\"\"\"\n","        probas = np.array([model._sigmoid(np.dot(X, model.weights) + model.bias) for model in self.models])\n","        # Average probabilities across all models\n","        return np.mean(probas, axis=0)\n"],"metadata":{"id":"ouOeXwHvIn2L","executionInfo":{"status":"ok","timestamp":1726847511290,"user_tz":-360,"elapsed":11,"user":{"displayName":"Nafiul Islam","userId":"03319937839838894668"}}},"execution_count":282,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-fXZ1nOBZpwg"},"source":["**bold text**"]},{"cell_type":"markdown","source":["# Logistic Regression"],"metadata":{"id":"dzZLrWQ-u8IE"}},{"cell_type":"code","source":["import numpy as np\n","\n","class MyLogisticRegression:\n","    def __init__(self, learning_rate=0.3, n_iterations=1000):\n","        self.learning_rate = learning_rate\n","        self.n_iterations = n_iterations\n","        self.weights = None\n","        self.bias = None\n","\n","    def _sigmoid(self, z):\n","        \"\"\"Apply the sigmoid function.\"\"\"\n","        return 1 / (1 + np.exp(-z))\n","\n","    def _initialize_parameters(self, n_features):\n","        \"\"\"Initialize weights and bias to zero.\"\"\"\n","        self.weights = np.zeros(n_features)\n","        self.bias = 0\n","\n","    def _compute_loss(self, y_true, y_predicted):\n","        \"\"\"Compute the binary cross-entropy loss.\"\"\"\n","        n_samples = len(y_true)\n","        # Loss = - 1/n * Σ [y * log(p) + (1 - y) * log(1 - p)]\n","        loss = (-1 / n_samples) * np.sum(y_true * np.log(y_predicted) + (1 - y_true) * np.log(1 - y_predicted))\n","        return loss\n","\n","    def _update_weights(self, X, y_true, y_predicted):\n","        \"\"\"Perform a single update of gradient descent.\"\"\"\n","        n_samples = X.shape[0]\n","        # Compute the gradients\n","        dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y_true))\n","        db = (1 / n_samples) * np.sum(y_predicted - y_true)\n","\n","        # Update weights and bias\n","        self.weights -= self.learning_rate * dw\n","        self.bias -= self.learning_rate * db\n","\n","    def fit(self, X, y):\n","        \"\"\"Fit the logistic regression model to the data.\"\"\"\n","        X = np.asarray(X, dtype=np.float64)\n","        y = np.asarray(y, dtype=np.float64)\n","        n_samples, n_features = X.shape\n","\n","        # Initialize weights and bias\n","        self._initialize_parameters(n_features)\n","\n","        # Gradient descent\n","        for i in range(self.n_iterations):\n","            # Linear model\n","            linear_model = np.dot(X, self.weights) + self.bias\n","            # Apply sigmoid function\n","            y_predicted = self._sigmoid(linear_model)\n","\n","            # Compute the loss (optional, but useful for monitoring)\n","            loss = self._compute_loss(y, y_predicted)\n","            # if i % 100 == 0:\n","            #     print(f\"Iteration {i}, Loss: {loss}\")\n","\n","            # Update weights and bias using gradient descent\n","            self._update_weights(X, y, y_predicted)\n","\n","    def predict(self, X):\n","        \"\"\"Predict class labels for samples in X.\"\"\"\n","        X = np.asarray(X, dtype=np.float64)\n","        linear_model = np.dot(X, self.weights) + self.bias\n","        y_predicted = self._sigmoid(linear_model)\n","        y_predicted_labels = [1 if i > 0.5 else 0 for i in y_predicted]\n","        return np.array(y_predicted_labels)\n","\n","\n","# Example usage:\n","# if __name__ == \"__main__\":\n","#     # Sample data (features and labels)\n","#     X = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])\n","#     y = np.array([0, 0, 0, 1, 1])\n","\n","#     # Create and train the model\n","#     model = MyLogisticRegression(learning_rate=0.01, n_iterations=1000)\n","#     model.fit(X, y)\n","\n","#     # Make predictions\n","#     predictions = model.predict(X)\n","#     print(\"Predictions:\", predictions)\n"],"metadata":{"id":"pOAyhfjhx_3u","executionInfo":{"status":"ok","timestamp":1726847511290,"user_tz":-360,"elapsed":10,"user":{"displayName":"Nafiul Islam","userId":"03319937839838894668"}}},"execution_count":283,"outputs":[]},{"cell_type":"markdown","source":["# Stacking"],"metadata":{"id":"HAqA6H0S-G0I"}},{"cell_type":"code","source":["import numpy as np\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, average_precision_score\n","\n","class MyStackingClassifier:\n","    def __init__(self, base_estimators, meta_estimator):\n","        self.base_estimators = base_estimators\n","        self.meta_estimator = meta_estimator\n","\n","    def fit(self, X_train, y_train, X_val, y_val):\n","        \"\"\"Fit the base models and then the meta-learner.\"\"\"\n","        meta_features = []\n","\n","        # Train each base estimator on the training data\n","        for estimator in self.base_estimators:\n","            estimator.fit(X_train, y_train)\n","            # Generate meta-features by making predictions on the validation set\n","            meta_features.append(estimator.predict_proba(X_val))\n","\n","        # Stack the meta-features\n","        meta_features = np.column_stack(meta_features)\n","\n","        # Train the meta-learner on the meta-features using the validation labels\n","        self.meta_estimator.fit(meta_features, y_val)\n","\n","    def predict(self, X_test):\n","        \"\"\"Make predictions on the test set using the base models and the meta-learner.\"\"\"\n","        # Get meta-features for the test set by making predictions using the base models\n","        meta_features = []\n","        for estimator in self.base_estimators:\n","            meta_features.append(estimator.predict_proba(X_test))\n","\n","        # Stack the meta-features\n","        meta_features = np.column_stack(meta_features)\n","\n","        # Make final predictions using the meta-learner\n","        return self.meta_estimator.predict(meta_features)\n","\n","    def evaluate(self, X_test, y_test):\n","        \"\"\"Evaluate the model and calculate the required metrics.\"\"\"\n","        # Predict using the test data\n","        y_pred = self.predict(X_test)\n","\n","        # Compute the metrics\n","        accuracy = accuracy_score(y_test, y_pred)\n","        precision = precision_score(y_test, y_pred)\n","        recall = recall_score(y_test, y_pred)\n","        f1 = f1_score(y_test, y_pred)\n","        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()  # true negatives, false positives, false negatives, true positives\n","        specificity = tn / (tn + fp)\n","        auc = roc_auc_score(y_test, y_pred)\n","        aupr = average_precision_score(y_test, y_pred)\n","\n","        # Print the metrics\n","        print(\"Accuracy:\", accuracy)\n","        print(\"Precision:\", precision)\n","        print(\"Recall:\", recall)\n","        print(\"F1 Score:\", f1)\n","        print(\"Specificity:\", specificity)\n","        print(\"AUC ROC:\", auc)\n","        print(\"AUPR:\", aupr)\n","\n","        # Return the metrics as a dictionary (optional)\n","        return {\n","            'accuracy': accuracy,\n","            'precision': precision,\n","            'recall': recall,\n","            'f1_score': f1,\n","            'specificity': specificity,\n","            'auc_roc': auc,\n","            'aupr': aupr\n","        }\n","\n","# Example usage\n","# Create an instance of MyStackingClassifier (make sure you define your\n"],"metadata":{"id":"fpQviEcW-FCk","executionInfo":{"status":"ok","timestamp":1726847511290,"user_tz":-360,"elapsed":10,"user":{"displayName":"Nafiul Islam","userId":"03319937839838894668"}}},"execution_count":284,"outputs":[]},{"cell_type":"markdown","source":["# Violine Plot"],"metadata":{"id":"RbVzh1s1T4_w"}},{"cell_type":"code","source":["# Function to calculate multiple performance metrics\n","def evaluate_model_performance(y_true, y_pred):\n","    \"\"\"Calculate accuracy, precision, recall, F1, specificity, AUC-ROC, and AUC-PR.\"\"\"\n","    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n","\n","    accuracy = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred)\n","    recall = recall_score(y_true, y_pred)\n","    f1 = f1_score(y_true, y_pred)\n","    specificity = tn / (tn + fp)\n","    auc = roc_auc_score(y_true, y_pred)\n","    aupr = average_precision_score(y_true, y_pred)\n","\n","    return accuracy, precision, recall, f1, specificity, auc, aupr\n","\n","# Function to collect metrics from all 9 base learners\n","def collect_metrics_from_bagging(bagging_classifier, X_test, y_test):\n","    metrics_list = []\n","    for model in bagging_classifier.models:\n","        y_pred = model.predict(X_test)\n","        accuracy, precision, recall, f1, specificity, auc, aupr = evaluate_model_performance(y_test, y_pred)\n","        metrics_list.append([accuracy, f1, recall, precision, specificity, auc, aupr])\n","    return metrics_list\n","\n","# Function to plot the violin plot for all metrics across the 9 learners\n","def plot_violin_for_metrics(metrics_list):\n","    # Convert the metrics list to a pandas DataFrame\n","    metrics_df = pd.DataFrame(metrics_list, columns=[\"Accuracy\", \"F1\", \"Sensitivity\", \"Precision\", \"Specificity\", \"AUROC\", \"AUPR\"])\n","\n","    # Plot using seaborn's violinplot\n","    plt.figure(figsize=(12, 8))\n","    sns.violinplot(data=metrics_df, inner=\"point\")\n","    plt.title(\"Performance Metrics of Bagged Logistic Regression Models\")\n","    plt.xlabel(\"Metric\")\n","    plt.ylabel(\"Metric Score\")\n","    plt.xticks(ticks=np.arange(len(metrics_df.columns)), labels=metrics_df.columns)  # Ensure proper labeling\n","    plt.show()"],"metadata":{"id":"8iqHBnN_T7NO","executionInfo":{"status":"ok","timestamp":1726847511290,"user_tz":-360,"elapsed":9,"user":{"displayName":"Nafiul Islam","userId":"03319937839838894668"}}},"execution_count":285,"outputs":[]},{"cell_type":"code","source":["# Import necessary libraries\n","import numpy as np\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","# Assume X and y are your input matrices\n","# X --> (number of rows, number of columns), already scaled\n","# y --> binary target class (0 or 1)\n","# Dummy example data (replace these with your actual data)\n","# X = np.random.rand(100, 5) # Example feature matrix with 100 rows and 5 columns\n","# y = np.random.randint(0, 2, 100) # Example binary target vector\n","# Step 1: Split the data into training and testing sets\n","# X = np.array(my_20_features)\n","# y = np.array(target)\n","X_train, X_test, y_train, y_test = preprocess_credit_data()\n","X_trainBag, X_val, y_trainBag, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n","# Create bagging classifiers with MyLogisticRegression as the base model\n","bagging_classifiers = [MyBaggingClassifier(base_estimator=MyLogisticRegression, n_estimators=9) for _ in range(3)]\n","\n","# Use MyBaggingClassifier exactly as named in your code\n","bagging_classifier = MyBaggingClassifier(base_estimator=MyLogisticRegression, n_estimators=9)\n","\n","# Train the bagging classifier\n","bagging_classifier.fit(X_train, y_train)\n","\n","# Collect the performance metrics from all the 9 base learners\n","metrics_list = collect_metrics_from_bagging(bagging_classifier, X_test, y_test)\n","\n","# Plot the violin plot for the metrics collected from the bagging classifier\n","plot_violin_for_metrics(metrics_list)\n","\n","# Meta-learner\n","meta_learner = MyLogisticRegression()\n","\n","# Create stacking classifier\n","stacking_classifier = MyStackingClassifier(base_estimators=bagging_classifiers, meta_estimator=meta_learner)\n","\n","# Train the stacking classifier using the validation set as meta-training data\n","stacking_classifier.fit(X_trainBag, y_trainBag, X_val, y_val)\n","stacking_classifier.evaluate(X_test, y_test)\n","# Make predictions on the test set\n","y_pred = stacking_classifier.predict(X_test)\n","# print(\"Test Predictions:\", y_pred)\n","\n","# # Compare predictions with true values\n","# print(\"True Test Labels:\", y_test)\n","\n","# bagging_accuracies = []\n","# for classifier in bagging_classifiers:\n","#     accuracy = accuracy_score(y_test, classifier.predict(X_test))\n","#     bagging_accuracies.append(accuracy)\n","\n","# # Plot the violin plot for the bagging accuracies\n","# plot_violin(bagging_accuracies)\n","\n","# bagging_classifier = MyBaggingClassifier(base_estimator=MyLogisticRegression, n_estimators=9)\n","# bagging_classifier.fit(X_trainBag, y_trainBag)\n","# predictions = bagging_classifier.predict(X_val)\n","# print(\"Validation Predictions:\", predictions)\n","\n","# # Compare predictions with true values\n","# print(\"True Validation Labels:\", y_val)\n","\n","# y_pred = bagging_classifier.predict(X_val)\n","accuracy = accuracy_score(y_test, y_pred)\n","# print(f\"Accuracy of Bagging classifier: {accuracy:.2f}\"\n","\n","\n","# # Step 2: Initialize the Logistic Regression classifier\n","# clf =  MyLogisticRegression(learning_rate=0.01, n_iterations=1000)\n","# # Step 3: Train the classifier on the training data\n","# clf.fit(X_train, y_train)\n","# # Step 4: Make predictions on the test set\n","# y_pred = clf.predict(X_test)\n","# # Step 5: Evaluate the classifier's performance\n","# accuracy = accuracy_score(y_test, y_pred)\n","print(f\"Accuracy of Logistic Regression classifier: {accuracy:.2f}\")"],"metadata":{"id":"k_z0EtP3yKGI","executionInfo":{"status":"error","timestamp":1726847530512,"user_tz":-360,"elapsed":5352,"user":{"displayName":"Nafiul Islam","userId":"03319937839838894668"}},"colab":{"base_uri":"https://localhost:8080/","height":515},"outputId":"acee3985-9f45-4ffb-cdbe-218663cd8a71"},"execution_count":287,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset dimensions: (284807, 31)\n","Total missing values: 0\n","Duplicated rows: 1081\n","Binary columns: ['Class']\n","Nominal columns: []\n","Columns to scale: ['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']\n","Subset dimensions: (20473, 31)\n"]},{"output_type":"error","ename":"KeyError","evalue":"\"None of [Index([ 3794, 13421, 10724,  7932,  5306, 15390,  9562, 10470, 14596, 12396,\\n       ...\\n        1212, 11220, 10501, 10067,  4435, 11311,  2353,  6995, 14091, 11996],\\n      dtype='int64', length=16378)] are in the [columns]\"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-287-61ebc9ffa28a>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Train the bagging classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mbagging_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Collect the performance metrics from all the 9 base learners\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-282-aef261e7b8b4>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;31m# Generate bootstrap sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mX_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bootstrap_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;31m# Train a new model on the bootstrap sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-282-aef261e7b8b4>\u001b[0m in \u001b[0;36m_bootstrap_sample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3897\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3898\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3899\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3901\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6113\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6115\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6117\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6174\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muse_interval_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6175\u001b[0m                     \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6176\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6178\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: \"None of [Index([ 3794, 13421, 10724,  7932,  5306, 15390,  9562, 10470, 14596, 12396,\\n       ...\\n        1212, 11220, 10501, 10067,  4435, 11311,  2353,  6995, 14091, 11996],\\n      dtype='int64', length=16378)] are in the [columns]\""]}]},{"cell_type":"markdown","source":["# Table generation"],"metadata":{"id":"GGlTB7Y-CE0g"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score, confusion_matrix\n","\n","# Function to calculate multiple performance metrics\n","def evaluate_model_performance(y_true, y_pred):\n","    \"\"\"Calculate accuracy, precision, recall, F1, specificity, AUC-ROC, and AUC-PR.\"\"\"\n","    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n","\n","    accuracy = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred)\n","    recall = recall_score(y_true, y_pred)\n","    f1 = f1_score(y_true, y_pred)\n","    specificity = tn / (tn + fp)\n","    auc = roc_auc_score(y_true, y_pred)\n","    aupr = average_precision_score(y_true, y_pred)\n","\n","    return [accuracy, precision, recall, f1, specificity, auc, aupr]\n","\n","# Function to collect metrics from all base learners in the bagging model\n","def collect_metrics_from_bagging(bagging_classifier, X_test, y_test):\n","    metrics_list = []\n","    for model in bagging_classifier.models:\n","        y_pred = model.predict(X_test)\n","        metrics = evaluate_model_performance(y_test, y_pred)\n","        metrics_list.append(metrics)\n","    return metrics_list\n","\n","# Function to compute average and std deviation for bagged learners\n","def summarize_bagging_performance(metrics_list):\n","    metrics_array = np.array(metrics_list)\n","    means = metrics_array.mean(axis=0)\n","    stds = metrics_array.std(axis=0)\n","    return means, stds\n","\n","# Function to create the performance table\n","def create_performance_table(models_metrics):\n","    # Create a pandas DataFrame to display results\n","    metrics_df = pd.DataFrame(models_metrics, columns=[\"Model\", \"Accuracy\", \"Sensitivity\", \"Specificity\", \"Precision\", \"F1-score\", \"AUROC\", \"AUPR\"])\n","    return metrics_df\n","\n","def custom_majority_voting(base_estimators, X_test):\n","\n","    # Collect predictions from each base estimator\n","    predictions = np.array([estimator.predict(X_test) for estimator in base_estimators])\n","\n","    # Apply majority voting by taking the most common prediction (mode) along axis 0\n","    majority_votes = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=predictions)\n","\n","    return majority_votes\n","\n","# Collect performance metrics for different models\n","models_metrics = []\n","\n","# Bagging LR learners (Assuming `bagging_classifier` is trained and has 9 learners)\n","bagging_classifier = MyBaggingClassifier(base_estimator=MyLogisticRegression, n_estimators=9)\n","bagging_classifier.fit(X_train, y_train)  # Don't forget to fit the bagging classifier\n","bagging_metrics_list = collect_metrics_from_bagging(bagging_classifier, X_test, y_test)\n","bagging_means, bagging_stds = summarize_bagging_performance(bagging_metrics_list)\n","\n","# Append LR bagging metrics (mean ± std)\n","models_metrics.append([\n","    f\"LR (Bagging)\",\n","    f\"{bagging_means[0]:.4f} ± {bagging_stds[0]:.4f}\",  # Accuracy\n","    f\"{bagging_means[2]:.4f} ± {bagging_stds[2]:.4f}\",  # Sensitivity\n","    f\"{bagging_means[4]:.4f} ± {bagging_stds[4]:.4f}\",  # Specificity\n","    f\"{bagging_means[1]:.4f} ± {bagging_stds[1]:.4f}\",  # Precision\n","    f\"{bagging_means[3]:.4f} ± {bagging_stds[3]:.4f}\",  # F1-score\n","    f\"{bagging_means[5]:.4f} ± {bagging_stds[5]:.4f}\",  # AUROC\n","    f\"{bagging_means[6]:.4f} ± {bagging_stds[6]:.4f}\"   # AUPR\n","])\n","\n","# Custom Voting Ensemble (Using the custom majority voting function)\n","models = bagging_classifier.models\n","y_pred_voting = custom_majority_voting(models, X_test)\n","voting_metrics = evaluate_model_performance(y_test, y_pred_voting)\n","models_metrics.append([\"Voting Ensemble\"] + [f\"{metric:.4f}\" for metric in voting_metrics])\n","\n","# Stacking ensemble (Assuming `stacking_classifier` is trained)\n","stacking_metrics = stacking_classifier.evaluate(X_test, y_test)\n","models_metrics.append([\n","    \"Stacking Ensemble\",\n","    f\"{stacking_metrics['accuracy']:.4f}\",\n","    f\"{stacking_metrics['recall']:.4f}\",   # Sensitivity (Recall)\n","    f\"{stacking_metrics['specificity']:.4f}\",\n","    f\"{stacking_metrics['precision']:.4f}\",\n","    f\"{stacking_metrics['f1_score']:.4f}\",\n","    f\"{stacking_metrics['auc_roc']:.4f}\",\n","    f\"{stacking_metrics['aupr']:.4f}\"\n","])\n","\n","# Create the performance table\n","performance_table = create_performance_table(models_metrics)\n","\n","\n","performance_table\n"],"metadata":{"id":"SON7H81lCCq5","executionInfo":{"status":"aborted","timestamp":1726847516485,"user_tz":-360,"elapsed":11,"user":{"displayName":"Nafiul Islam","userId":"03319937839838894668"}}},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"},"colab":{"provenance":[{"file_id":"10UDOxtkdx6b85KRvm1wiH0zANezSHq8m","timestamp":1726722971223}]}},"nbformat":4,"nbformat_minor":0}